{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e6d99d",
   "metadata": {},
   "source": [
    "# Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b205ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Transformer, Geod\n",
    "from tqdm import tqdm\n",
    "# Ignore copy warning\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e7b42",
   "metadata": {},
   "source": [
    "# Infrastructure point data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cfa351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON point data files \n",
    "\n",
    "with open('london_infrastructure/cycle_parking.json', 'r') as f:\n",
    "    cycle_parking_json = json.loads(f.read())\n",
    "cycle_parking = pd.json_normalize(cycle_parking_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/restricted_point.json', 'r') as f:\n",
    "    restricted_point_json = json.loads(f.read())\n",
    "restricted_point = pd.json_normalize(restricted_point_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/signage.json', 'r') as f:\n",
    "    signage_json = json.loads(f.read())\n",
    "signage = pd.json_normalize(signage_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/signal.json', 'r') as f:\n",
    "    signal_json = json.loads(f.read())\n",
    "signal = pd.json_normalize(signal_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/traffic_calming.json', 'r') as f:\n",
    "    traffic_calming_json = json.loads(f.read())\n",
    "traffic_calming = pd.json_normalize(traffic_calming_json, record_path = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41851802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv files \n",
    "cycle_parking.to_csv('london_infrastructure/cycle_parking.csv', index=False)\n",
    "restricted_point.to_csv('london_infrastructure/restricted_point.csv', index=False)\n",
    "signage.to_csv('london_infrastructure/signage.csv', index=False)\n",
    "signal.to_csv('london_infrastructure/signal.csv', index=False)\n",
    "traffic_calming.to_csv('london_infrastructure/traffic_calming.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ef110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new csv files\n",
    "cycle_parking = pd.read_csv('london_infrastructure/cycle_parking.csv')\n",
    "restricted_point = pd.read_csv('london_infrastructure/restricted_point.csv')\n",
    "signage = pd.read_csv('london_infrastructure/signage.csv', low_memory=False)\n",
    "signal = pd.read_csv('london_infrastructure/signal.csv')\n",
    "traffic_calming = pd.read_csv('london_infrastructure/traffic_calming.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db91a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process dataframes \n",
    "\n",
    "def process_file(file):\n",
    "    \n",
    "    # Drop unnecessary columns, rename remaining columns\n",
    "    file.drop(['type', 'geometry.type', 'properties.FEATURE_ID', 'properties.PHOTO1_URL', 'properties.PHOTO2_URL'],\n",
    "              axis=1, inplace=True)\n",
    "    file.rename(columns={'geometry.coordinates':'coordinates', 'properties.SVDATE':'date_of_survey', \n",
    "                         'properties.BOROUGH':'borough'}, inplace=True)\n",
    "    \n",
    "    # Clean coordinate data\n",
    "    file['coordinates'] = file['coordinates'].str.replace('[\\[\\]]', '', regex=True)\n",
    "    file['latitude'] = [file['coordinates'][i].split(', ')[1] for i in range(file.shape[0])]\n",
    "    file['longitude'] = [file['coordinates'][i].split(', ')[0] for i in range(file.shape[0])]\n",
    "    \n",
    "    # Remove duplicate points (keep most recent)\n",
    "    file.sort_values(by='date_of_survey', ascending=True, inplace=True)\n",
    "    file.drop_duplicates(subset=['latitude', 'longitude'], inplace=True, keep='last')\n",
    "    \n",
    "    # Return clean file  \n",
    "    file.drop(['coordinates', 'date_of_survey'], axis=1, inplace=True)\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0971289a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process dataframes \n",
    "restricted_point = process_file(restricted_point)\n",
    "signal = process_file(signal)\n",
    "traffic_calming = process_file(traffic_calming)\n",
    "cycle_parking = process_file(cycle_parking)\n",
    "signage = process_file(signage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2868210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process data further \n",
    "\n",
    "# Drop columns with majority null values \n",
    "signage.drop(['properties.SS_ROUTEN', 'properties.SS_ACCESS'], axis=1, inplace=True)\n",
    "\n",
    "# Correct typo in True/False column  \n",
    "signage['properties.SS_CYCSMB'] = np.where(signage['properties.SS_CYCSMB']=='TRUE', True, False)\n",
    "\n",
    "# Set unknown or unexpected values to null  \n",
    "signage['properties.SS_COLOUR'] = signage['properties.SS_COLOUR'].replace('<Null>', np.nan)\n",
    "signage['properties.SS_NAME'] = signage['properties.SS_NAME'].replace([' ', 'UNKNOWN', 'TRUE'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55fe36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final versions \n",
    "cycle_parking.to_csv('london_infrastructure/cycle_parking.csv', index=False)\n",
    "restricted_point.to_csv('london_infrastructure/restricted_point.csv', index=False)\n",
    "signage.to_csv('london_infrastructure/signage.csv', index=False)\n",
    "signal.to_csv('london_infrastructure/signal.csv', index=False)\n",
    "traffic_calming.to_csv('london_infrastructure/traffic_calming.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68943956",
   "metadata": {},
   "source": [
    "# Infrastructure line data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6006a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON line data files \n",
    "\n",
    "with open('london_infrastructure/advanced_stop_line.json', 'r') as f:\n",
    "    advanced_stop_line_json = json.loads(f.read())\n",
    "advanced_stop_line = pd.json_normalize(advanced_stop_line_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/crossing.json', 'r') as f:\n",
    "    crossing_json = json.loads(f.read())\n",
    "crossing = pd.json_normalize(crossing_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/cycle_lane_track.json', 'r') as f:\n",
    "    cycle_lane_track_json = json.loads(f.read())\n",
    "cycle_lane_track = pd.json_normalize(cycle_lane_track_json, record_path = ['features'])\n",
    "\n",
    "with open('london_infrastructure/restricted_route.json', 'r') as f:\n",
    "    restricted_route_json = json.loads(f.read())\n",
    "restricted_route = pd.json_normalize(restricted_route_json, record_path = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba00674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv files \n",
    "advanced_stop_line.to_csv('london_infrastructure/advanced_stop_line.csv', index=False)\n",
    "crossing.to_csv('london_infrastructure/crossing.csv', index=False)\n",
    "cycle_lane_track.to_csv('london_infrastructure/cycle_lane_track.csv', index=False)\n",
    "restricted_route.to_csv('london_infrastructure/restricted_route.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f475f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new csv files\n",
    "advanced_stop_line = pd.read_csv('london_infrastructure/advanced_stop_line.csv')\n",
    "crossing = pd.read_csv('london_infrastructure/crossing.csv')\n",
    "cycle_lane_track = pd.read_csv('london_infrastructure/cycle_lane_track.csv')\n",
    "restricted_route = pd.read_csv('london_infrastructure/restricted_route.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a55e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate list of coordinates into separate rows \n",
    "def separate_coordinates(df):\n",
    "    id_list = []\n",
    "    coord_list = []\n",
    "    for i in range(df.shape[0]):\n",
    "        split_coords = df['geometry.coordinates'][i].split('],')\n",
    "        for j in range(len(split_coords)):\n",
    "            id_list.append(df['properties.FEATURE_ID'][i])\n",
    "            coord_list.append(split_coords[j])\n",
    "    df_split = pd.DataFrame(zip(id_list, coord_list)).rename(columns={0:'properties.FEATURE_ID', 1:'coordinates'})\n",
    "    df = pd.merge(df_split, df, on='properties.FEATURE_ID', how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9362fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate coordinates \n",
    "crossing = separate_coordinates(crossing)\n",
    "advanced_stop_line = separate_coordinates(advanced_stop_line)\n",
    "restricted_route = separate_coordinates(restricted_route)\n",
    "cycle_lane_track = separate_coordinates(cycle_lane_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083f4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process dataframes \n",
    "\n",
    "def process_file(file):\n",
    "    \n",
    "    # Drop unnecessary columns, rename remaining columns\n",
    "    file.drop(['type', 'geometry.type', 'geometry.coordinates', 'properties.FEATURE_ID', \n",
    "               'properties.PHOTO1_URL', 'properties.PHOTO2_URL'], axis=1, inplace=True)\n",
    "    file.rename(columns={'properties.SVDATE':'date_of_survey', 'properties.BOROUGH':'borough'}, inplace=True)\n",
    "    \n",
    "    # Clean coordinate data\n",
    "    file['coordinates'] = file['coordinates'].str.replace('[\\[\\]]', '', regex=True)\n",
    "    file['latitude'] = [file['coordinates'][i].split(', ')[1] for i in range(file.shape[0])]\n",
    "    file['longitude'] = [file['coordinates'][i].split(', ')[0] for i in range(file.shape[0])]\n",
    "    \n",
    "    # Remove duplicate points (keep most recent)\n",
    "    file.sort_values(by='date_of_survey', ascending=True, inplace=True)\n",
    "    file.drop_duplicates(subset=['latitude', 'longitude'], inplace=True, keep='last')\n",
    "    \n",
    "    # Return clean file  \n",
    "    file.drop(['coordinates', 'date_of_survey'], axis=1, inplace=True)\n",
    "    file.reset_index(drop=True, inplace=True)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e2310d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process dataframes \n",
    "crossing = process_file(crossing)\n",
    "advanced_stop_line = process_file(advanced_stop_line)\n",
    "restricted_route = process_file(restricted_route)\n",
    "cycle_lane_track = process_file(cycle_lane_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b759793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data further \n",
    "\n",
    "# Drop columns with majority null values \n",
    "cycle_lane_track.drop(['properties.CLT_ACCESS'], axis=1, inplace=True)\n",
    "\n",
    "# Set unknown or unexpected values to null  \n",
    "cycle_lane_track['properties.CLT_SHARED'] = cycle_lane_track['properties.CLT_SHARED'].replace('TCB', np.nan)\n",
    "cycle_lane_track['properties.CLT_MANDAT'] = cycle_lane_track['properties.CLT_MANDAT'].replace('TCB', np.nan)\n",
    "cycle_lane_track['properties.CLT_PRIORI'] = cycle_lane_track['properties.CLT_PRIORI'].replace('TRE', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b20ca142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final versions \n",
    "crossing.to_csv('london_infrastructure/crossing.csv', index=False)\n",
    "advanced_stop_line.to_csv('london_infrastructure/advanced_stop_line.csv', index=False)\n",
    "restricted_route.to_csv('london_infrastructure/restricted_route.csv', index=False)\n",
    "cycle_lane_track.to_csv('london_infrastructure/cycle_lane_track.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b69de9",
   "metadata": {},
   "source": [
    "# Create counts by borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a982939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign names to infrastructure dataframes \n",
    "restricted_point.attrs['name'] = 'restricted_point'\n",
    "signal.attrs['name'] = 'signal'\n",
    "cycle_parking.attrs['name'] = 'cycle_parking'\n",
    "traffic_calming.attrs['name'] = 'traffic_calming'\n",
    "signage.attrs['name'] = 'signage'\n",
    "crossing.attrs['name'] = 'crossing'\n",
    "advanced_stop_line.attrs['name'] = 'advanced_stop_line'\n",
    "restricted_route.attrs['name'] = 'restricted_route'\n",
    "cycle_lane_track.attrs['name'] = 'cycle_lane_track'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919d440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each infrastructure surveyed in each borough\n",
    "\n",
    "borough_df = pd.DataFrame()\n",
    "\n",
    "for df in [cycle_parking, restricted_point, signage, signal, traffic_calming, crossing, \n",
    "           advanced_stop_line, restricted_route, cycle_lane_track]:\n",
    "    df_count = df.groupby(['borough']).count()[['latitude']].rename(\n",
    "        columns={'latitude': f\"{df.attrs['name']}_count\"})\n",
    "    borough_df = pd.concat([borough_df, df_count], axis=1)\n",
    "\n",
    "borough_df.fillna(0, inplace=True)\n",
    "borough_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c3470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "borough_df.to_csv('london_infrastructure/borough_infrastructure.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f18e68",
   "metadata": {},
   "source": [
    "# Bike sites data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c3defd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load site data\n",
    "sites = pd.read_excel('London/Biking sites.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8853ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Easting/Northing to Latitude/Longitude converter\n",
    "transformer = Transformer.from_crs('epsg:27700', 'epsg:4326')\n",
    "\n",
    "# Get coordinates of sites \n",
    "sites['coordinates'] = ''\n",
    "for i in range(sites.shape[0]):\n",
    "    sites['coordinates'][i] = transformer.transform(sites.Easting[i], sites.Northing[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635d3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distances to infrastructure features  \n",
    "\n",
    "def distance_features(df):\n",
    "\n",
    "    closest = []\n",
    "    count_100m = []\n",
    "    count_1000m = []\n",
    "\n",
    "    for i in tqdm(range(sites.shape[0])):\n",
    "        distance = []\n",
    "        for j in range(df.shape[0]):    \n",
    "            distance.append(Geod(ellps='WGS84').inv(sites.coordinates[i][1], sites.coordinates[i][0], \n",
    "                                             df.longitude[j], df.latitude[j])[2]) \n",
    "        closest.append(min(distance))\n",
    "        count_100m.append(len([m for m in distance if m<=100]))\n",
    "        count_1000m.append(len([m for m in distance if m<=1000]))\n",
    "\n",
    "    sites[f\"closest_{df.attrs['name']}\"] = closest\n",
    "    sites[f\"{df.attrs['name']}_count_100m\"] = count_100m\n",
    "    sites[f\"{df.attrs['name']}_count_1000m\"] = count_1000m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0db07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [00:22<00:00, 90.39it/s] \n"
     ]
    }
   ],
   "source": [
    "distance_features(restricted_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d7353e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [00:48<00:00, 41.34it/s]\n"
     ]
    }
   ],
   "source": [
    "distance_features(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57932a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [07:47<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "distance_features(crossing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4965c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [14:09<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "distance_features(advanced_stop_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0250e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [1:23:52<00:00,  2.49s/it]   \n"
     ]
    }
   ],
   "source": [
    "distance_features(restricted_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9f868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2023/2023 [5:47:43<00:00, 10.31s/it]     \n"
     ]
    }
   ],
   "source": [
    "distance_features(cycle_parking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7695d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this updated version \n",
    "sites.to_csv('London/updated_sites.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25917645",
   "metadata": {},
   "source": [
    "# For the future \n",
    "- Include traffic_calming, signage and cycle_lane_track \n",
    "- Include additional features from each dataframe "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
